{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12742,
     "status": "ok",
     "timestamp": 1554987090878,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "OUex7kp0xOPZ",
    "outputId": "c9230bc1-5aa0-45b8-dfbf-12d04f47dc26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sickle in /usr/local/lib/python3.6/dist-packages (0.6.4)\n",
      "Requirement already satisfied: lxml>=3.2.3 in /usr/local/lib/python3.6/dist-packages (from sickle) (4.2.6)\n",
      "Requirement already satisfied: requests>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from sickle) (2.18.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->sickle) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->sickle) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->sickle) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=1.1.0->sickle) (2.6)\n",
      "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
      "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.33.1)\n",
      "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.11)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.6.9)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.10)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.14.6)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.12.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (7.0.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.11.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (40.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.5.3)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install sickle\n",
    "!pip3 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10264,
     "status": "ok",
     "timestamp": 1554986892919,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "xL8tGMB5xO4K",
    "outputId": "55183996-5bf6-4d33-9bd7-42c66216432d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2535,
     "status": "ok",
     "timestamp": 1554988444050,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "XO3CqiHTxdfx",
    "outputId": "d7d36a8a-37ad-479c-9a4e-6b1ef9f0c799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/RecRes/final\n"
     ]
    }
   ],
   "source": [
    "cd '/content/drive/My Drive/RecRes/final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1712,
     "status": "ok",
     "timestamp": 1554988444053,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "bJvH2aSWxgsR",
    "outputId": "e3f89308-ddeb-4791-e2f7-50fe70932356"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/RecRes/final'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2153,
     "status": "ok",
     "timestamp": 1554988470847,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "TkE3IUMYw4ve",
    "outputId": "3c763fb9-c17a-4736-829f-e960c1890b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sickle import Sickle\n",
    "import os\n",
    "import pickle\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sickle.iterator import OAIResponseIterator\n",
    "import re\n",
    "\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import string\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import _pickle as cPickle\n",
    "\n",
    "import json\n",
    "import marshal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_QCnoZAw4vs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sickle = Sickle('http://export.arxiv.org/oai2', iterator = OAIResponseIterator)\n",
    "responses = sickle.ListRecords(**{'metadataPrefix' : 'oai_dc', 'set' : 'cs',\n",
    "                                  'from': '2019-03-21', 'until' : '2019-03-21'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arXqcM3kw4vx"
   },
   "outputs": [],
   "source": [
    "def oai_harvester_response(metadataPrefix = 'oai_dc', harvest_set = 'cs', ):\n",
    "    '''\n",
    "        harvest_from : of format 'yyyy-mm-dd'\n",
    "        harvest_from : of format 'yyyy-mm-dd'\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    from sickle import Sickle\n",
    "    import numpy as np\n",
    "    import extract\n",
    "    import os\n",
    "    import pickle\n",
    "    from datetime import date\n",
    "    from sickle.iterator import OAIResponseIterator\n",
    "    '''\n",
    "    \n",
    "    sickle = Sickle('http://export.arxiv.org/oai2', iterator = OAIResponseIterator)\n",
    "            \n",
    "    try:\n",
    "        with open('./pkl/today.pickle', 'rb') as handle:\n",
    "            today = pickle.load(handle)\n",
    "            \n",
    "            if(today!= str(date.today())):\n",
    "                responses = sickle.ListRecords(**{'metadataPrefix' : metadataPrefix, 'set' : harvest_set,\n",
    "                                          'from': today, 'until' : str(date.today() - relativedelta(days = 1))})\n",
    "                handle.close()\n",
    "                \n",
    "            else:\n",
    "                print('The records are already present till yesterday!')\n",
    "                responses = None\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "            responses = sickle.ListRecords(**{'metadataPrefix' : metadataPrefix, 'set' : harvest_set})\n",
    "                                             #,'from' : str(date.today)})\n",
    "                \n",
    "    except NoRecordsMatchError:\n",
    "        responses = None\n",
    "        print('There are no more records to be updated!')\n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1904,
     "status": "ok",
     "timestamp": 1554994488511,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "n9jhnO7mw4v1",
    "outputId": "3761bc2c-ff74-442e-d60b-098bcddc302a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The records are already present till yesterday!\n"
     ]
    }
   ],
   "source": [
    "responses = oai_harvester_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ouRD2Qdw4v8"
   },
   "outputs": [],
   "source": [
    "def oai_harvester_dumper(responses):   \n",
    "    try:\n",
    "        with open('./pkl/flag.pickle', 'rb') as handle1, open('./pkl/today.pickle', 'rb') as handle2:\n",
    "            flag = pickle.load(handle1)\n",
    "            today = pickle.load(handle2)\n",
    "            handle1.close()\n",
    "            handle2.close()\n",
    "            \n",
    "            if((today!= str(date.today())) & (responses!= None)):\n",
    "                try:\n",
    "                    while(True):\n",
    "                        flag = flag + 1\n",
    "                        with open('./xml/response' + str(flag) + '.xml', 'w') as fp:\n",
    "                            raw = responses.next().raw\n",
    "                            if(raw != None):\n",
    "                                fp.write(raw)\n",
    "                                print('Wrote #', flag)\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "                except StopIteration:\n",
    "                    # if StopIteration is raised, break from loop\n",
    "                    with open('./pkl/today.pickle', 'wb') as handle:\n",
    "                        pickle.dump(str(date.today()), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        handle.close()\n",
    "                    with open('./pkl/flag.pickle', 'wb') as handle:\n",
    "                        pickle.dump(flag-1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        handle.close()\n",
    "                    os.remove('./xml/response' + str(flag) + '.xml')\n",
    "                    return\n",
    "            else:\n",
    "                print('No responses exists to dump!' )\n",
    "                \n",
    "                \n",
    "    except:\n",
    "        flag = -1\n",
    "        print('Here')\n",
    "        \n",
    "        if(responses!= None):\n",
    "            try:\n",
    "                while(True):\n",
    "                    flag = flag + 1\n",
    "                    with open('./xml/response' + str(flag) + '.xml', 'w') as fp:\n",
    "\n",
    "                        raw = responses.next().raw\n",
    "                        #print(raw)\n",
    "\n",
    "                        if(raw != None):\n",
    "                            fp.write(raw)\n",
    "                            print('Wrote ', flag, '.xml')\n",
    "\n",
    "            except StopIteration:\n",
    "                    # if StopIteration is raised, break from loop\n",
    "                if(flag > -1):\n",
    "                    os.remove('./xml/response' + str(flag) + '.xml')\n",
    "\n",
    "                with open('./pkl/today.pickle', 'wb') as handle:\n",
    "                    pickle.dump(str(date.today()), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    handle.close()\n",
    "\n",
    "                with open('./pkl/flag.pickle', 'wb') as handle:\n",
    "                    pickle.dump(flag-1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    handle.close()\n",
    "                return\n",
    "        else:\n",
    "                 print('No responses exists to dump!' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJDNEB_LWqg7"
   },
   "outputs": [],
   "source": [
    "'''with open('./pkl/today.pickle', 'wb') as handle:\n",
    "    pickle.dump('2019-04-08', handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()\n",
    "\n",
    "with open('./pkl/flag.pickle', 'wb') as handle:\n",
    "    pickle.dump(209, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3042,
     "status": "ok",
     "timestamp": 1554992800139,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "s0hGJ_P7w4wJ",
    "outputId": "1b9e933a-2b13-4899-d44e-c5ee3a9400a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote # 211\n"
     ]
    }
   ],
   "source": [
    "oai_harvester_dumper(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9B5d2v_mw4wN"
   },
   "outputs": [],
   "source": [
    "def dictRecords(xmlfile, flag1):\n",
    "    \n",
    "    #Shorthands\n",
    "    OAI = '{http://www.openarchives.org/OAI/2.0/}'\n",
    "    OAI2 = '{http://www.openarchives.org/OAI/2.0/oai_dc/}'\n",
    "    PURL = '{http://purl.org/dc/elements/1.1/}'\n",
    "    \n",
    "    #create element tree object\n",
    "    tree = ET.parse(xmlfile)\n",
    "    \n",
    "    #get root element\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for item in root.findall('./' + OAI + 'ListRecords/'):\n",
    "        #print(flag1)\n",
    "        flag1 = flag1 + 1\n",
    "        recDict[flag1] = {}\n",
    " \n",
    "        for idfier in item.findall('./'+ OAI + 'header/' + OAI + 'identifier'):\n",
    "            recDict[flag1]['id'] = re.sub('\\\\n', '', idfier.text)\n",
    "            id_list.append(recDict[flag1]['id'])\n",
    "            #absDict['rec' + str(flag1)]['id'] = recDict['rec' + str(flag1)]['id']\n",
    "            \n",
    "        '''for datestamp in item.findall('./'+ OAI + 'header/' + OAI + 'datestamp'):\n",
    "            recDict[flag1]['datestamp'] = re.sub('\\\\n', '', datestamp.text)\n",
    "'''\n",
    "        for metadata in item.findall('./' + OAI + 'metadata'):\n",
    "            #print(metadata.getchildren())\n",
    "            for dc in metadata.findall('./' + OAI2 + 'dc'):\n",
    "\n",
    "                #For storing the title of the record\n",
    "                for title in dc.findall('./' + PURL + 'title'):\n",
    "                    recDict[flag1]['title'] = title.text\n",
    "                    title_list.append(preprocess_dict(recDict[flag1]['title']).split())\n",
    "                    #recDict[flag1]['title'] = re.sub('\\\\n', '', title.text)\n",
    "                    #absDict['rec' + str(flag1)]['title'] = recDict['rec' + str(flag1)]['title']\n",
    "                    \n",
    "                '''#For storing the creator(s) of the record\n",
    "                recDict[flag1]['creator'] = []\n",
    "                for creator in dc.findall('./' + PURL + 'creator'):\n",
    "                    recDict[flag1]['creator'].append(re.sub('\\\\n', '', creator.text))\n",
    "'''\n",
    "                '''#For storing the subject(s) this record belongs to\n",
    "                recDict['rec' + str(flag1)]['subject'] = []\n",
    "                for subject in dc.findall('./' + PURL + 'subject'):\n",
    "                    recDict['rec' + str(flag1)]['subject'].append(re.sub('\\\\n', '', subject.text))\n",
    "                '''\n",
    "\n",
    "                #For storing the processed_description (abstract) of the record\n",
    "                i = 0\n",
    "                for descrip in dc.findall('./' + PURL + 'description'):\n",
    "                    i += 1\n",
    "                    if(i == 1):\n",
    "                        #recDict['rec' + str(flag1)]['abstract'] = re.sub('\\\\n', '', descrip.text)\n",
    "                        txt = re.sub('\\\\n', ' ', descrip.text)\n",
    "                        #absDict['rec' + str(flag1)]['abstract'] = txt\n",
    "                        #recDict['rec' + str(flag1)]['abstract'] = preprocess_dict(txt)\n",
    "                        recDict[flag1]['abstract'] = txt\n",
    "                        abstract_list.append(preprocess_dict(recDict[flag1]['abstract']).split())\n",
    "                        \n",
    "    if(len(recDict) != len(abstract_list)):\n",
    "        recDict.pop(flag1, None)\n",
    "        return recDict, flag1 - 1, abstract_list, id_list, title_list\n",
    "    else:\n",
    "        return recDict, flag1, abstract_list, id_list, title_list\n",
    "    #_ = abstract_list.pop()\n",
    "    #_ = id_list.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6036,
     "status": "ok",
     "timestamp": 1554988455830,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "FwHRAtufTmUK",
    "outputId": "82b76172-2c52-404e-9b48-956e050fc075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run like bitch on a highway'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#example text \n",
    "text = 'running like bitches on a highway'\n",
    "\n",
    "class Splitter(object):\n",
    "    \"\"\"\n",
    "    split the document into sentences and tokenize each sentence\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self,text):\n",
    "        \"\"\"\n",
    "        out : ['What', 'can', 'I', 'say', 'about', 'this', 'place', '.']\n",
    "        \"\"\"\n",
    "        # split into single sentence\n",
    "        sentences = self.splitter.tokenize(text)\n",
    "        # tokenization in each sentences\n",
    "        tokens = [self.tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class LemmatizationWithPOSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_wordnet_pos(self,treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def pos_tag(self,tokens):\n",
    "        # find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
    "        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "\n",
    "        # lemmatization using pos tagg   \n",
    "        # convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ... ie [original WORD, Lemmatized word, POS tag]\n",
    "        pos_tokens = [ [(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)), [pos_tag]) for (word,pos_tag) in pos] for pos in pos_tokens]\n",
    "        return pos_tokens\n",
    "      \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "splitter = Splitter()\n",
    "lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n",
    "\n",
    "#step 1 split document into sentence followed by tokenization\n",
    "tokens = splitter.split(text)\n",
    "\n",
    "#step 2 lemmatization using pos tagger \n",
    "lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)\n",
    "tokens = [lemma_pos_token[0][i][1] for i in range(len(lemma_pos_token[0]))]\n",
    "' '.join([token for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HRbAkgww4wT"
   },
   "outputs": [],
   "source": [
    "def preprocess_dict(text):\n",
    "    \n",
    "    #Removing instances like (SGD), (ANN) etc\n",
    "    text = re.sub(r'[A-Z]{2,}', '', text)\n",
    "\n",
    "    #Removing instances like (SGD), (ANN) etc\n",
    "    text = re.sub(r'[A-Z]+-[A-Z]+', '', text)\n",
    "    \n",
    "    #Removing instances like SGD, ANN etc\n",
    "    text = re.sub(r'\\([A-Z]{2,}\\)', '', text)\n",
    "\n",
    "    #Removing instances like AB-CDE, EF-GHI etc\n",
    "    text = re.sub(r'\\([A-Z]+-[A-Z]+\\)', '', text)\n",
    "\n",
    "    #Removing instances (i), (ii), (iii)\n",
    "    text = re.sub(r'\\([i]{1,2,3}\\)', '', text)\n",
    "\n",
    "    #Removing instances (1), (2), (3)\n",
    "    text = re.sub(r'\\([\\d]+\\)', '', text)\n",
    "        \n",
    "    #Removing Mathemeatical formulaes using Latex like representation\n",
    "    text = re.sub(r'[$].*[$]', '', text)\n",
    "\n",
    "    #Removing XML Escape Characters\n",
    "    text = re.sub(r'&amp;', '', text)\n",
    "    text = re.sub(r'&lt;', ' ', text)\n",
    "    text = re.sub(r'&gt;', ' ', text)\n",
    "    text = re.sub(r'&quot;', '', text)\n",
    "    text = re.sub(r'&apos;', '', text)\n",
    "    \n",
    "    #Lowering case the text\n",
    "    text = text.lower()\n",
    "        \n",
    "    #Removing numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    #Removing words of length 1\n",
    "    text = re.sub(r'\\b\\w{1}\\b', '', text)\n",
    "    \n",
    "    #Removing - from pair of words\n",
    "    text = text.replace('-', \" \")\n",
    "    \n",
    "    #Removing punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #Removing stop words\n",
    "    tokens = word_tokenize(text)\n",
    "    text = ' '.join([ i for i in tokens if not i in stopwords])\n",
    "    \n",
    "    #Lemmatizing using POS Tags\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    splitter = Splitter()\n",
    "    lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()\n",
    "\n",
    "    tokens = splitter.split(text)    #step 1 split document into sentence followed by tokenization\n",
    "    lemma_pos_token = lemmatization_using_pos_tagger.pos_tag(tokens)#step 2 lemmatization using pos tagger \n",
    "    try:\n",
    "        tokens = [lemma_pos_token[0][i][1] for i in range(len(lemma_pos_token[0]))]\n",
    "        text = ' '.join([token for token in tokens])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #Removing stop words (post lemmatization)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    text = ' '.join([ i for i in tokens if not i in stopwords])\n",
    "   \n",
    "    #Removing apostrophes\n",
    "    text = text.replace('”', \"\")\n",
    "    text = text.replace('“', \"\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31260,
     "status": "ok",
     "timestamp": 1554995004939,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "joyYM5Pxw4wb",
    "outputId": "8b1231d6-23ed-4587-ed1a-62df7432f7d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files = pd.Series(glob.glob('./xml/*.xml'))\n",
    "file_nums = files.str.findall(r'response(\\d+)\\.xml').apply(np.squeeze).astype('uint16').sort_values().reset_index(drop = True)\n",
    "\n",
    "\n",
    "try:\n",
    "    with open('./pkl/fileDumpCounter.pickle', 'rb') as handle:\n",
    "        fileDumpCounter = pickle.load(handle)\n",
    "        handle.close()\n",
    "        \n",
    "        nums = file_nums[file_nums > fileDumpCounter]\n",
    "        #print(nums)        \n",
    "        if(len(nums) != 0):\n",
    "            with open('./pkl/recDict.pickle', 'rb') as handle1, open('./pkl/flag1.pickle', 'rb') as handle2, open('./pkl/abstract_list.pickle', 'rb') as handle3, open('./pkl/id_list.pickle', 'rb') as handle4, open('./pkl/title_list.pickle', 'rb') as handle5:\n",
    "                recDict = cPickle.load(handle1)\n",
    "                flag1 = cPickle.load(handle2)\n",
    "                abstract_list = cPickle.load(handle3)\n",
    "                id_list = cPickle.load(handle4)\n",
    "                title_list = pickle.load(handle5)\n",
    "\n",
    "            handle1.close()\n",
    "            handle2.close()\n",
    "            handle3.close()\n",
    "            handle4.close()\n",
    "            handle5.close()\n",
    "\n",
    "            for num in tqdm.tqdm(nums):\n",
    "                #fileDumpCounter += 1\n",
    "                recDict, flag1, abstract_list, id_list, title_list = dictRecords('./xml/response' + str(num) + '.xml', flag1)            \n",
    "        try:\n",
    "            fileDumpCounter = max(nums)\n",
    "            \n",
    "            #flag1 is the actual record number in the dictionary. Record nums : [0, inf]\n",
    "            #flag1 is equal to  len(recDict - 1)\n",
    "\n",
    "            with open('./pkl/recDict.pickle', 'wb') as handle1:\n",
    "                cPickle.dump(recDict, handle1, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/flag1.pickle', 'wb') as handle2:\n",
    "                cPickle.dump(flag1, handle2, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/abstract_list.pickle', 'wb') as handle3:\n",
    "                cPickle.dump(abstract_list, handle3, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/id_list.pickle', 'wb') as handle4:\n",
    "                cPickle.dump(id_list, handle4, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/fileDumpCounter.pickle', 'wb') as handle5:\n",
    "                    pickle.dump(fileDumpCounter, handle5, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/title_list.pickle', 'wb') as handle6:\n",
    "                    pickle.dump(title_list, handle6, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "            handle1.close()\n",
    "            handle2.close()\n",
    "            handle3.close()\n",
    "            handle4.close()\n",
    "            handle5.close()\n",
    "            handle6.close()\n",
    "                        \n",
    "        except ValueError:\n",
    "            print('Dumping file counter!')\n",
    "            with open('./pkl/fileDumpCounter.pickle', 'wb') as handle:\n",
    "                    pickle.dump(fileDumpCounter, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            handle.close()\n",
    "\n",
    "except:\n",
    "        #fileDumpCounter = -1\n",
    "        print('No prev dict found. Creating a new one!')\n",
    "        recDict = {}\n",
    "        flag1 = -1\n",
    "        abstract_list = []\n",
    "        id_list = []\n",
    "        title_list = []\n",
    "        \n",
    "        for num in tqdm.tqdm(file_nums):\n",
    "            recDict, flag1, abstract_list, id_list, title_list = dictRecords('./xml/response' + str(num) + '.xml', flag1)\n",
    "\n",
    "        try:\n",
    "            fileDumpCounter = max(file_nums)\n",
    "            \n",
    "            with open('./pkl/recDict.pickle', 'wb') as handle1:\n",
    "                cPickle.dump(recDict, handle1, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/flag1.pickle', 'wb') as handle2:\n",
    "                cPickle.dump(flag1, handle2, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/abstract_list.pickle', 'wb') as handle3:\n",
    "                cPickle.dump(abstract_list, handle3, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/id_list.pickle', 'wb') as handle4:\n",
    "                cPickle.dump(id_list, handle4, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/fileDumpCounter.pickle', 'wb') as handle5:\n",
    "                    pickle.dump(fileDumpCounter, handle5, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            with open('./pkl/title_list.pickle', 'wb') as handle6:\n",
    "                    pickle.dump(title_list, handle6, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "            handle1.close()\n",
    "            handle2.close()\n",
    "            handle3.close()\n",
    "            handle4.close()\n",
    "            handle5.close()\n",
    "            handle6.close()\n",
    "            \n",
    "        except ValueError:\n",
    "            print('No .xml files to write into dictionary!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXZjbfumw4xa"
   },
   "outputs": [],
   "source": [
    "def pklInfo():\n",
    "    \n",
    "    with open('./pkl/flag.pickle', 'rb') as handle:\n",
    "        flag = pickle.load(handle)\n",
    "        print('flag : ', flag)\n",
    "        handle.close()\n",
    "        \n",
    "    with open('./pkl/today.pickle', 'rb') as handle:\n",
    "        today = pickle.load(handle)\n",
    "        print('today : ', today)\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/flag1.pickle', 'rb') as handle:\n",
    "        flag1 = pickle.load(handle)\n",
    "        print('flag1 : ', flag1)\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/recDict.pickle', 'rb') as handle:\n",
    "        recDict = pickle.load(handle)\n",
    "        print('Length of recDict : ', len(recDict))\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/abstract_list.pickle', 'rb') as handle:\n",
    "        abstract_list = pickle.load(handle)\n",
    "        print('Length of abstract_list : ', len(abstract_list))\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/id_list.pickle', 'rb') as handle:\n",
    "        id_list = pickle.load(handle)\n",
    "        print('Length of id_list : ', len(id_list))\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/fileDumpCounter.pickle', 'rb') as handle:\n",
    "        fileDumpCounter = pickle.load(handle)\n",
    "        print('fileDumpCounter : ', fileDumpCounter)\n",
    "        handle.close()    \n",
    "    \n",
    "    with open('./pkl/title_list.pickle', 'rb') as handle:\n",
    "        title_list = pickle.load(handle)\n",
    "        print('Length of title_list : ', len(title_list))\n",
    "        handle.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17439,
     "status": "ok",
     "timestamp": 1554995014799,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "OTl6ElREw4xc",
    "outputId": "dbed4439-139d-470f-9521-c796fe7b7346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag :  211\n",
      "today :  2019-04-11\n",
      "flag1 :  208933\n",
      "Length of recDict :  208934\n",
      "Length of abstract_list :  208934\n",
      "Length of id_list :  208934\n",
      "fileDumpCounter :  211\n",
      "Length of title_list :  208934\n"
     ]
    }
   ],
   "source": [
    "pklInfo()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Harvester - Responser, Dumper(RecDict).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
