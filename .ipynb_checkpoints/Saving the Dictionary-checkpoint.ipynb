{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 986,
     "status": "ok",
     "timestamp": 1554992934964,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "3OJD6ARLMaVt",
    "outputId": "be65b00d-5918-4b65-cdbd-cb8957376116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3188,
     "status": "ok",
     "timestamp": 1554998162232,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "6DnPn0ntMRsQ",
    "outputId": "cd1f96a7-49e0-4038-ef0e-770562551f0f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import os\n",
    "import pickle as pickle\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "#remove common words and tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sickle import Sickle\n",
    "import os\n",
    "import pickle\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sickle.iterator import OAIResponseIterator\n",
    "import re\n",
    "\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import string\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import _pickle as cPickle\n",
    "\n",
    "import json\n",
    "import marshal\n",
    "\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3146,
     "status": "ok",
     "timestamp": 1554998162234,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "ZmVqecz7MjFb",
    "outputId": "69de73be-8a60-4d45-921b-380f783f3fb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3125,
     "status": "ok",
     "timestamp": 1554998162236,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "x1F1YiQgMmIl",
    "outputId": "ad18589e-60d5-49d5-e613-cbbf8a197d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/RecRes/final\n"
     ]
    }
   ],
   "source": [
    "cd '/content/drive/My Drive/RecRes/final/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gso9IIfSMRsW"
   },
   "outputs": [],
   "source": [
    "try:    \n",
    "    with open('./pkl/recDict.pickle', 'rb') as handle:\n",
    "        recDict = pickle.load(handle)\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/abstract_list.pickle', 'rb') as handle:\n",
    "        abstract_list = pickle.load(handle)\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/id_list.pickle', 'rb') as handle:\n",
    "        id_list = pickle.load(handle)\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/title_list.pickle', 'rb') as handle:\n",
    "        title_list = pickle.load(handle)\n",
    "        handle.close()\n",
    "        \n",
    "    with open('./pkl/flag1.pickle', 'rb') as handle:\n",
    "        flag1 = pickle.load(handle)\n",
    "        handle.close()        \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print('One or more of the required files missing')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykz2geXmMRsZ"
   },
   "outputs": [],
   "source": [
    "combined_list = [a + b for a, b in zip(abstract_list, title_list)]\n",
    "\n",
    "with open('./pkl/combined_list.pickle', 'wb') as handle:\n",
    "  pickle.dump(combined_list, handle, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60087,
     "status": "ok",
     "timestamp": 1554997737394,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "kHBt1qruMRsc",
    "outputId": "604adcb5-4466-4587-dd92-deb07d342746"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-11 15:47:58,031 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-11 15:47:59,099 : INFO : adding document #10000 to Dictionary(21819 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:00,267 : INFO : adding document #20000 to Dictionary(31403 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:01,388 : INFO : adding document #30000 to Dictionary(39225 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:02,560 : INFO : adding document #40000 to Dictionary(45980 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:03,733 : INFO : adding document #50000 to Dictionary(52205 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:04,980 : INFO : adding document #60000 to Dictionary(57851 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:06,173 : INFO : adding document #70000 to Dictionary(63104 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:07,405 : INFO : adding document #80000 to Dictionary(67716 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:08,653 : INFO : adding document #90000 to Dictionary(72468 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:09,933 : INFO : adding document #100000 to Dictionary(76919 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:11,237 : INFO : adding document #110000 to Dictionary(81214 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:12,597 : INFO : adding document #120000 to Dictionary(85463 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:13,966 : INFO : adding document #130000 to Dictionary(89464 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:15,343 : INFO : adding document #140000 to Dictionary(93583 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:16,738 : INFO : adding document #150000 to Dictionary(97712 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:18,114 : INFO : adding document #160000 to Dictionary(101539 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:19,524 : INFO : adding document #170000 to Dictionary(105370 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:20,919 : INFO : adding document #180000 to Dictionary(109123 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:22,346 : INFO : adding document #190000 to Dictionary(112690 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:23,614 : INFO : adding document #200000 to Dictionary(116825 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...)\n",
      "2019-04-11 15:48:24,707 : INFO : built Dictionary(119662 unique tokens: ['algorithm', 'algorithms', 'also', 'certify', 'color']...) from 208934 documents (total 19444593 corpus positions)\n",
      "2019-04-11 15:48:46,269 : INFO : saving Dictionary object under ./text/combined.dict, separately None\n",
      "2019-04-11 15:48:46,386 : INFO : saved ./text/combined.dict\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "\n",
    "try:\n",
    "    with open('./pkl/flag2.pickle', 'rb') as handle1, open('./text/corpus.pickle', 'rb') as handle2:\n",
    "      \n",
    "      flag2 = pickle.load(handle1)\n",
    "      if(combined_list[flag2 + 1 : ]):\n",
    "            corpus = pickle.load(handle2)\n",
    "            handle2.close()\n",
    "\n",
    "            dictionary = corpora.Dictionary.load('./text/combined.dict')\n",
    "            dictionary.add_documents(combined_list[flag2 + 1:])\n",
    "\n",
    "            corpus.extend([dictionary.doc2bow(text) for text in combined_list[flag2 + 1 :]])\n",
    "            print('Corpus updated successfully')\n",
    "      \n",
    "      handle1.close()\n",
    "            \n",
    "except:\n",
    "    if(combined_list):\n",
    "        dictionary = corpora.Dictionary(combined_list)\n",
    "        corpus = [dictionary.doc2bow(text) for text in combined_list]\n",
    "        \n",
    "\n",
    "dictionary.save('./text/combined.dict')\n",
    "\n",
    "with open('./text/corpus.pickle', 'wb') as handle:\n",
    "    pickle.dump(corpus, handle, pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()\n",
    "\n",
    "with open('./pkl/flag2.pickle', 'wb') as handle:\n",
    "    pickle.dump(len(combined_list), handle, pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6157,
     "status": "ok",
     "timestamp": 1554997845483,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "5EBr_jhDMRsg",
    "outputId": "bc0ebf1d-95cf-416d-9399-23771bd77ed6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-11 15:50:39,796 : INFO : collecting document frequencies\n",
      "2019-04-11 15:50:39,799 : INFO : PROGRESS: processing document #0\n",
      "2019-04-11 15:50:39,983 : INFO : PROGRESS: processing document #10000\n",
      "2019-04-11 15:50:40,182 : INFO : PROGRESS: processing document #20000\n",
      "2019-04-11 15:50:40,388 : INFO : PROGRESS: processing document #30000\n",
      "2019-04-11 15:50:40,601 : INFO : PROGRESS: processing document #40000\n",
      "2019-04-11 15:50:40,821 : INFO : PROGRESS: processing document #50000\n",
      "2019-04-11 15:50:41,035 : INFO : PROGRESS: processing document #60000\n",
      "2019-04-11 15:50:41,250 : INFO : PROGRESS: processing document #70000\n",
      "2019-04-11 15:50:41,466 : INFO : PROGRESS: processing document #80000\n",
      "2019-04-11 15:50:41,687 : INFO : PROGRESS: processing document #90000\n",
      "2019-04-11 15:50:41,914 : INFO : PROGRESS: processing document #100000\n",
      "2019-04-11 15:50:42,141 : INFO : PROGRESS: processing document #110000\n",
      "2019-04-11 15:50:42,362 : INFO : PROGRESS: processing document #120000\n",
      "2019-04-11 15:50:42,587 : INFO : PROGRESS: processing document #130000\n",
      "2019-04-11 15:50:42,841 : INFO : PROGRESS: processing document #140000\n",
      "2019-04-11 15:50:43,080 : INFO : PROGRESS: processing document #150000\n",
      "2019-04-11 15:50:43,320 : INFO : PROGRESS: processing document #160000\n",
      "2019-04-11 15:50:43,561 : INFO : PROGRESS: processing document #170000\n",
      "2019-04-11 15:50:43,814 : INFO : PROGRESS: processing document #180000\n",
      "2019-04-11 15:50:44,058 : INFO : PROGRESS: processing document #190000\n",
      "2019-04-11 15:50:44,283 : INFO : PROGRESS: processing document #200000\n",
      "2019-04-11 15:50:44,475 : INFO : calculating IDF weights for 208934 documents and 119661 features (13324107 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) #Step 1 --> Initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2261,
     "status": "ok",
     "timestamp": 1554997850229,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "nqSsgRN2MRsk",
    "outputId": "9beffc5c-5809-4c2e-d5e8-7636b8ee0b54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208934"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2a5ExPL5IKZ"
   },
   "outputs": [],
   "source": [
    "def pklInfo():\n",
    "    \n",
    "    with open('./pkl/flag.pickle', 'rb') as handle:\n",
    "        flag = pickle.load(handle)\n",
    "        print('flag : ', flag)\n",
    "        handle.close()\n",
    "        \n",
    "    with open('./pkl/today.pickle', 'rb') as handle:\n",
    "        today = pickle.load(handle)\n",
    "        print('today : ', today)\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/flag1.pickle', 'rb') as handle:\n",
    "        flag1 = pickle.load(handle)\n",
    "        print('flag1 : ', flag1)\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/recDict.pickle', 'rb') as handle:\n",
    "        recDict = pickle.load(handle)\n",
    "        print('Length of recDict : ', len(recDict))\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/abstract_list.pickle', 'rb') as handle:\n",
    "        abstract_list = pickle.load(handle)\n",
    "        print('Length of abstract_list : ', len(abstract_list))\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/id_list.pickle', 'rb') as handle:\n",
    "        id_list = pickle.load(handle)\n",
    "        print('Length of id_list : ', len(id_list))\n",
    "        handle.close()\n",
    "\n",
    "    with open('./pkl/fileDumpCounter.pickle', 'rb') as handle:\n",
    "        fileDumpCounter = pickle.load(handle)\n",
    "        print('fileDumpCounter : ', fileDumpCounter)\n",
    "        handle.close()    \n",
    "    \n",
    "    with open('./pkl/title_list.pickle', 'rb') as handle:\n",
    "        title_list = pickle.load(handle)\n",
    "        print('Length of title_list : ', len(title_list))\n",
    "        handle.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9297,
     "status": "ok",
     "timestamp": 1554995044532,
     "user": {
      "displayName": "AMAN JAIN",
      "photoUrl": "https://lh4.googleusercontent.com/-6xExnnQAXkA/AAAAAAAAAAI/AAAAAAAAADg/28YDgB0hK1c/s64/photo.jpg",
      "userId": "03907457390895428361"
     },
     "user_tz": -330
    },
    "id": "vO0TGMNg5Ipw",
    "outputId": "f3ae05e7-a806-4fdb-d038-ed48dcfa324d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag :  211\n",
      "today :  2019-04-11\n",
      "flag1 :  208933\n",
      "Length of recDict :  208934\n",
      "Length of abstract_list :  208934\n",
      "Length of id_list :  208934\n",
      "fileDumpCounter :  211\n",
      "Length of title_list :  208934\n"
     ]
    }
   ],
   "source": [
    "pklInfo()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Saving the Dictionary.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
