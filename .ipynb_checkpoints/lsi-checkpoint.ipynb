{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import os\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "#remove common words and tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string\n",
    "stopwords = set(stopwords.words('english'))\n",
    " \n",
    "from gensim import models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sickle import Sickle\n",
    "import os\n",
    "import pickle\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sickle.iterator import OAIResponseIterator\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import string\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recDict.pickle', 'rb') as handle1, open('id_list.pickle') as handle2:\n",
    "    recDict = pickle.load(handle1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-29 16:33:14,851 : INFO : loading Dictionary object from abstract.dict\n",
      "2019-03-29 16:33:15,023 : INFO : loaded abstract.dict\n",
      "2019-03-29 16:33:15,076 : INFO : loaded corpus index from corpus.mm.index\n",
      "2019-03-29 16:33:15,078 : INFO : initializing cython corpus reader from corpus.mm\n",
      "2019-03-29 16:33:15,081 : INFO : accepted corpus with 205912 documents, 165200 features, 13915054 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "if(os.path.exists('abstract.dict')):\n",
    "    dictionary = corpora.Dictionary.load('abstract.dict')\n",
    "    corpus = corpora.MmCorpus('corpus.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TfIdf Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-29 16:33:17,960 : INFO : collecting document frequencies\n",
      "2019-03-29 16:33:17,963 : INFO : PROGRESS: processing document #0\n",
      "2019-03-29 16:33:18,628 : INFO : PROGRESS: processing document #10000\n",
      "2019-03-29 16:33:19,424 : INFO : PROGRESS: processing document #20000\n",
      "2019-03-29 16:33:20,204 : INFO : PROGRESS: processing document #30000\n",
      "2019-03-29 16:33:21,023 : INFO : PROGRESS: processing document #40000\n",
      "2019-03-29 16:33:21,799 : INFO : PROGRESS: processing document #50000\n",
      "2019-03-29 16:33:22,713 : INFO : PROGRESS: processing document #60000\n",
      "2019-03-29 16:33:23,588 : INFO : PROGRESS: processing document #70000\n",
      "2019-03-29 16:33:24,557 : INFO : PROGRESS: processing document #80000\n",
      "2019-03-29 16:33:25,515 : INFO : PROGRESS: processing document #90000\n",
      "2019-03-29 16:33:26,443 : INFO : PROGRESS: processing document #100000\n",
      "2019-03-29 16:33:27,360 : INFO : PROGRESS: processing document #110000\n",
      "2019-03-29 16:33:28,255 : INFO : PROGRESS: processing document #120000\n",
      "2019-03-29 16:33:29,108 : INFO : PROGRESS: processing document #130000\n",
      "2019-03-29 16:33:29,988 : INFO : PROGRESS: processing document #140000\n",
      "2019-03-29 16:33:30,880 : INFO : PROGRESS: processing document #150000\n",
      "2019-03-29 16:33:31,759 : INFO : PROGRESS: processing document #160000\n",
      "2019-03-29 16:33:32,637 : INFO : PROGRESS: processing document #170000\n",
      "2019-03-29 16:33:33,528 : INFO : PROGRESS: processing document #180000\n",
      "2019-03-29 16:33:34,431 : INFO : PROGRESS: processing document #190000\n",
      "2019-03-29 16:33:35,235 : INFO : PROGRESS: processing document #200000\n",
      "2019-03-29 16:33:35,664 : INFO : calculating IDF weights for 205912 documents and 165199 features (13915054 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) #Step 1 --> Initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-29 16:33:36,311 : INFO : using serial LSI version on this node\n",
      "2019-03-29 16:33:36,312 : INFO : updating model with new documents\n",
      "2019-03-29 16:33:41,929 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:33:42,691 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:33:42,692 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:33:43,653 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:33:59,467 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:34:00,375 : INFO : computing the final decomposition\n",
      "2019-03-29 16:34:00,376 : INFO : keeping 200 factors (discarding 16.465% of energy spectrum)\n",
      "2019-03-29 16:34:01,076 : INFO : processed documents up to #20000\n",
      "2019-03-29 16:34:01,159 : INFO : topic #0(17.762): 0.233*\"channel\" + 0.141*\"network\" + 0.140*\"code\" + 0.131*\"algorithm\" + 0.115*\"problem\" + 0.112*\"bound\" + 0.109*\"graph\" + 0.109*\"node\" + 0.109*\"rate\" + 0.106*\"capacity\"\n",
      "2019-03-29 16:34:01,166 : INFO : topic #1(11.407): 0.523*\"channel\" + 0.208*\"capacity\" + 0.205*\"relay\" + 0.185*\"interference\" + -0.172*\"graph\" + 0.144*\"rate\" + 0.132*\"receiver\" + -0.122*\"algorithm\" + 0.120*\"coding\" + 0.106*\"gaussian\"\n",
      "2019-03-29 16:34:01,172 : INFO : topic #2(9.469): -0.569*\"code\" + -0.192*\"graph\" + 0.188*\"network\" + -0.156*\"decoding\" + -0.148*\"bound\" + 0.139*\"node\" + 0.130*\"protocol\" + 0.120*\"user\" + 0.105*\"wireless\" + -0.101*\"ldpc\"\n",
      "2019-03-29 16:34:01,179 : INFO : topic #3(8.742): 0.530*\"code\" + -0.406*\"graph\" + -0.190*\"vertex\" + -0.136*\"edge\" + 0.127*\"decoding\" + -0.117*\"bound\" + -0.115*\"channel\" + -0.104*\"polynomial\" + 0.101*\"system\" + -0.099*\"problem\"\n",
      "2019-03-29 16:34:01,197 : INFO : topic #4(8.374): -0.397*\"node\" + -0.337*\"network\" + -0.249*\"graph\" + 0.226*\"channel\" + -0.217*\"code\" + -0.180*\"relay\" + -0.158*\"protocol\" + -0.150*\"routing\" + -0.143*\"packet\" + -0.139*\"wireless\"\n",
      "2019-03-29 16:34:07,736 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:34:08,662 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:34:08,663 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:34:09,650 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:34:26,795 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:34:27,586 : INFO : computing the final decomposition\n",
      "2019-03-29 16:34:27,587 : INFO : keeping 200 factors (discarding 16.862% of energy spectrum)\n",
      "2019-03-29 16:34:28,252 : INFO : merging projections: (165200, 200) + (165200, 200)\n",
      "2019-03-29 16:34:32,402 : INFO : keeping 200 factors (discarding 9.772% of energy spectrum)\n",
      "2019-03-29 16:34:33,536 : INFO : processed documents up to #40000\n",
      "2019-03-29 16:34:33,544 : INFO : topic #0(24.895): 0.171*\"channel\" + 0.147*\"network\" + 0.146*\"algorithm\" + 0.127*\"graph\" + 0.125*\"problem\" + 0.117*\"node\" + 0.112*\"code\" + 0.109*\"system\" + 0.102*\"bound\" + 0.096*\"time\"\n",
      "2019-03-29 16:34:33,552 : INFO : topic #1(15.059): -0.515*\"channel\" + -0.225*\"relay\" + 0.208*\"graph\" + -0.203*\"capacity\" + -0.185*\"interference\" + -0.159*\"rate\" + -0.139*\"coding\" + -0.130*\"receiver\" + -0.127*\"code\" + -0.119*\"scheme\"\n",
      "2019-03-29 16:34:33,560 : INFO : topic #2(13.033): 0.376*\"graph\" + 0.313*\"code\" + -0.181*\"network\" + 0.178*\"vertex\" + 0.178*\"bound\" + -0.123*\"protocol\" + -0.123*\"node\" + -0.120*\"system\" + -0.118*\"user\" + 0.118*\"edge\"\n",
      "2019-03-29 16:34:33,567 : INFO : topic #3(12.105): -0.447*\"code\" + 0.430*\"graph\" + 0.311*\"node\" + 0.240*\"network\" + 0.178*\"vertex\" + 0.152*\"edge\" + 0.133*\"relay\" + -0.108*\"decoding\" + 0.101*\"routing\" + 0.098*\"wireless\"\n",
      "2019-03-29 16:34:33,573 : INFO : topic #4(11.449): -0.594*\"code\" + -0.316*\"node\" + -0.249*\"network\" + 0.224*\"channel\" + 0.163*\"game\" + -0.136*\"protocol\" + -0.130*\"decoding\" + -0.121*\"routing\" + 0.114*\"interference\" + -0.102*\"sensor\"\n",
      "2019-03-29 16:34:40,747 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:34:41,658 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:34:41,659 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:34:42,657 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:34:58,369 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:34:59,354 : INFO : computing the final decomposition\n",
      "2019-03-29 16:34:59,355 : INFO : keeping 200 factors (discarding 17.258% of energy spectrum)\n",
      "2019-03-29 16:35:00,055 : INFO : merging projections: (165200, 200) + (165200, 200)\n",
      "2019-03-29 16:35:04,597 : INFO : keeping 200 factors (discarding 7.504% of energy spectrum)\n",
      "2019-03-29 16:35:05,747 : INFO : processed documents up to #60000\n",
      "2019-03-29 16:35:05,757 : INFO : topic #0(30.320): 0.152*\"algorithm\" + 0.149*\"network\" + 0.145*\"channel\" + 0.127*\"problem\" + 0.126*\"graph\" + 0.112*\"system\" + 0.109*\"node\" + 0.100*\"code\" + 0.098*\"model\" + 0.096*\"time\"\n",
      "2019-03-29 16:35:05,766 : INFO : topic #1(17.609): -0.500*\"channel\" + 0.224*\"graph\" + -0.216*\"relay\" + -0.191*\"capacity\" + -0.181*\"interference\" + -0.163*\"rate\" + -0.142*\"coding\" + -0.141*\"code\" + -0.134*\"scheme\" + -0.130*\"receiver\"\n",
      "2019-03-29 16:35:05,774 : INFO : topic #2(15.763): 0.423*\"graph\" + 0.246*\"code\" + 0.194*\"vertex\" + 0.181*\"bound\" + -0.151*\"network\" + 0.128*\"edge\" + -0.127*\"system\" + -0.125*\"user\" + -0.115*\"service\" + 0.105*\"polynomial\"\n",
      "2019-03-29 16:35:05,783 : INFO : topic #3(14.432): -0.415*\"code\" + 0.405*\"graph\" + 0.327*\"node\" + 0.290*\"network\" + 0.162*\"vertex\" + 0.139*\"edge\" + 0.122*\"relay\" + 0.112*\"wireless\" + 0.112*\"routing\" + 0.110*\"protocol\"\n",
      "2019-03-29 16:35:05,790 : INFO : topic #4(13.478): -0.641*\"code\" + -0.266*\"node\" + -0.222*\"network\" + 0.196*\"channel\" + 0.186*\"game\" + -0.125*\"decoding\" + -0.113*\"protocol\" + 0.103*\"interference\" + -0.099*\"routing\" + -0.087*\"sensor\"\n",
      "2019-03-29 16:35:15,067 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:35:15,858 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:35:15,859 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:35:16,768 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:35:34,453 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:35:35,532 : INFO : computing the final decomposition\n",
      "2019-03-29 16:35:35,533 : INFO : keeping 200 factors (discarding 17.260% of energy spectrum)\n",
      "2019-03-29 16:35:36,111 : INFO : merging projections: (165200, 200) + (165200, 200)\n",
      "2019-03-29 16:35:39,825 : INFO : keeping 200 factors (discarding 6.257% of energy spectrum)\n",
      "2019-03-29 16:35:40,941 : INFO : processed documents up to #80000\n",
      "2019-03-29 16:35:40,948 : INFO : topic #0(34.906): 0.152*\"algorithm\" + 0.147*\"network\" + 0.129*\"channel\" + 0.127*\"problem\" + 0.124*\"graph\" + 0.114*\"system\" + 0.103*\"node\" + 0.103*\"model\" + 0.097*\"time\" + 0.096*\"data\"\n",
      "2019-03-29 16:35:40,956 : INFO : topic #1(19.612): -0.482*\"channel\" + 0.246*\"graph\" + -0.210*\"relay\" + -0.181*\"capacity\" + -0.176*\"interference\" + -0.162*\"rate\" + -0.139*\"scheme\" + -0.136*\"code\" + -0.133*\"coding\" + -0.130*\"receiver\"\n",
      "2019-03-29 16:35:40,963 : INFO : topic #2(18.027): 0.432*\"graph\" + 0.224*\"code\" + 0.196*\"vertex\" + 0.184*\"bound\" + -0.132*\"network\" + 0.127*\"edge\" + -0.125*\"system\" + 0.122*\"channel\" + -0.121*\"user\" + -0.111*\"data\"\n",
      "2019-03-29 16:35:40,972 : INFO : topic #3(16.376): 0.399*\"code\" + -0.398*\"graph\" + -0.321*\"node\" + -0.301*\"network\" + -0.158*\"vertex\" + -0.134*\"edge\" + 0.119*\"matrix\" + -0.114*\"wireless\" + -0.106*\"relay\" + -0.105*\"protocol\"\n",
      "2019-03-29 16:35:40,978 : INFO : topic #4(15.154): -0.673*\"code\" + -0.223*\"node\" + -0.202*\"network\" + 0.152*\"channel\" + 0.151*\"game\" + -0.125*\"decoding\" + -0.106*\"graph\" + 0.099*\"problem\" + 0.098*\"signal\" + -0.090*\"protocol\"\n",
      "2019-03-29 16:35:47,519 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:35:48,278 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:35:48,279 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:35:49,090 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:36:03,775 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:36:04,599 : INFO : computing the final decomposition\n",
      "2019-03-29 16:36:04,600 : INFO : keeping 200 factors (discarding 17.154% of energy spectrum)\n",
      "2019-03-29 16:36:05,193 : INFO : merging projections: (165200, 200) + (165200, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-29 16:36:08,936 : INFO : keeping 200 factors (discarding 5.854% of energy spectrum)\n",
      "2019-03-29 16:36:10,034 : INFO : processed documents up to #100000\n",
      "2019-03-29 16:36:10,041 : INFO : topic #0(39.023): 0.151*\"algorithm\" + 0.148*\"network\" + 0.126*\"problem\" + 0.122*\"graph\" + 0.117*\"channel\" + 0.113*\"system\" + 0.108*\"model\" + 0.102*\"data\" + 0.097*\"time\" + 0.095*\"node\"\n",
      "2019-03-29 16:36:10,048 : INFO : topic #1(21.333): -0.473*\"channel\" + 0.254*\"graph\" + -0.195*\"relay\" + -0.177*\"capacity\" + -0.174*\"interference\" + -0.160*\"rate\" + -0.140*\"scheme\" + -0.134*\"code\" + -0.129*\"receiver\" + -0.124*\"coding\"\n",
      "2019-03-29 16:36:10,054 : INFO : topic #2(20.041): 0.451*\"graph\" + 0.200*\"vertex\" + 0.198*\"code\" + 0.183*\"bound\" + 0.129*\"edge\" + 0.118*\"channel\" + -0.114*\"network\" + -0.114*\"data\" + -0.113*\"system\" + -0.112*\"user\"\n",
      "2019-03-29 16:36:10,061 : INFO : topic #3(17.978): 0.401*\"code\" + -0.383*\"graph\" + -0.294*\"network\" + -0.286*\"node\" + -0.147*\"vertex\" + 0.133*\"matrix\" + -0.123*\"edge\" + 0.116*\"image\" + -0.111*\"energy\" + -0.111*\"wireless\"\n",
      "2019-03-29 16:36:10,067 : INFO : topic #4(16.585): -0.676*\"code\" + -0.162*\"network\" + -0.161*\"node\" + 0.125*\"signal\" + 0.119*\"algorithm\" + 0.114*\"channel\" + -0.113*\"decoding\" + -0.110*\"graph\" + 0.109*\"matrix\" + 0.104*\"problem\"\n",
      "2019-03-29 16:36:17,112 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:36:17,898 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:36:17,899 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:36:18,746 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:36:33,936 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:36:34,766 : INFO : computing the final decomposition\n",
      "2019-03-29 16:36:34,767 : INFO : keeping 200 factors (discarding 17.112% of energy spectrum)\n",
      "2019-03-29 16:36:35,343 : INFO : merging projections: (165200, 200) + (165200, 200)\n",
      "2019-03-29 16:36:39,098 : INFO : keeping 200 factors (discarding 5.521% of energy spectrum)\n",
      "2019-03-29 16:36:40,190 : INFO : processed documents up to #120000\n",
      "2019-03-29 16:36:40,197 : INFO : topic #0(42.791): 0.150*\"network\" + 0.147*\"algorithm\" + 0.124*\"problem\" + 0.118*\"graph\" + 0.114*\"model\" + 0.112*\"system\" + 0.108*\"data\" + 0.106*\"channel\" + 0.100*\"method\" + 0.096*\"time\"\n",
      "2019-03-29 16:36:40,205 : INFO : topic #1(22.824): -0.466*\"channel\" + 0.192*\"graph\" + -0.189*\"relay\" + -0.174*\"capacity\" + -0.166*\"interference\" + -0.162*\"rate\" + -0.157*\"code\" + -0.142*\"scheme\" + -0.128*\"receiver\" + -0.121*\"coding\"\n",
      "2019-03-29 16:36:40,211 : INFO : topic #2(21.899): 0.495*\"graph\" + 0.211*\"vertex\" + 0.171*\"bound\" + 0.155*\"code\" + 0.139*\"edge\" + -0.123*\"image\" + -0.118*\"user\" + 0.113*\"polynomial\" + 0.112*\"algorithm\" + -0.108*\"network\"\n",
      "2019-03-29 16:36:40,220 : INFO : topic #3(19.380): 0.415*\"code\" + -0.340*\"graph\" + -0.280*\"network\" + -0.257*\"node\" + 0.160*\"image\" + 0.129*\"matrix\" + -0.125*\"vertex\" + -0.122*\"user\" + -0.118*\"energy\" + -0.112*\"social\"\n",
      "2019-03-29 16:36:40,226 : INFO : topic #4(17.941): -0.496*\"code\" + 0.264*\"image\" + -0.173*\"game\" + -0.143*\"quantum\" + 0.135*\"signal\" + 0.133*\"channel\" + 0.107*\"method\" + 0.104*\"graph\" + -0.104*\"system\" + 0.102*\"sparse\"\n",
      "2019-03-29 16:36:47,640 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:36:48,435 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:36:48,436 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:36:49,679 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:37:07,225 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:37:08,232 : INFO : computing the final decomposition\n",
      "2019-03-29 16:37:08,233 : INFO : keeping 200 factors (discarding 16.820% of energy spectrum)\n",
      "2019-03-29 16:37:08,965 : INFO : merging projections: (165200, 200) + (165200, 200)\n",
      "2019-03-29 16:37:13,676 : INFO : keeping 200 factors (discarding 5.319% of energy spectrum)\n",
      "2019-03-29 16:37:15,119 : INFO : processed documents up to #140000\n",
      "2019-03-29 16:37:15,127 : INFO : topic #0(46.332): 0.153*\"network\" + 0.143*\"algorithm\" + 0.121*\"problem\" + 0.120*\"model\" + 0.114*\"graph\" + 0.113*\"data\" + 0.111*\"system\" + 0.105*\"method\" + 0.095*\"channel\" + 0.094*\"time\"\n",
      "2019-03-29 16:37:15,134 : INFO : topic #1(24.278): -0.402*\"channel\" + 0.194*\"image\" + -0.182*\"code\" + -0.156*\"relay\" + -0.154*\"capacity\" + -0.146*\"rate\" + -0.140*\"interference\" + -0.130*\"bound\" + -0.128*\"scheme\" + 0.122*\"learning\"\n",
      "2019-03-29 16:37:15,141 : INFO : topic #2(23.538): 0.539*\"graph\" + 0.220*\"vertex\" + -0.174*\"channel\" + 0.151*\"edge\" + -0.147*\"user\" + 0.139*\"algorithm\" + 0.121*\"problem\" + 0.116*\"bound\" + 0.114*\"polynomial\" + -0.105*\"network\"\n",
      "2019-03-29 16:37:15,152 : INFO : topic #3(20.605): 0.377*\"code\" + -0.262*\"graph\" + -0.252*\"network\" + -0.220*\"node\" + 0.215*\"image\" + -0.145*\"user\" + 0.133*\"matrix\" + -0.128*\"social\" + 0.117*\"channel\" + -0.113*\"energy\"\n",
      "2019-03-29 16:37:15,160 : INFO : topic #4(19.437): -0.348*\"graph\" + -0.311*\"image\" + 0.202*\"code\" + 0.202*\"game\" + -0.180*\"network\" + -0.149*\"channel\" + 0.144*\"quantum\" + 0.128*\"system\" + -0.126*\"node\" + 0.126*\"logic\"\n",
      "2019-03-29 16:37:22,659 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:37:23,457 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:37:23,458 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:37:24,368 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:37:39,077 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:37:39,880 : INFO : computing the final decomposition\n",
      "2019-03-29 16:37:39,881 : INFO : keeping 200 factors (discarding 16.778% of energy spectrum)\n",
      "2019-03-29 16:37:40,462 : INFO : merging projections: (165200, 200) + (165200, 200)\n",
      "2019-03-29 16:37:44,667 : INFO : keeping 200 factors (discarding 4.822% of energy spectrum)\n",
      "2019-03-29 16:37:45,843 : INFO : processed documents up to #160000\n",
      "2019-03-29 16:37:45,850 : INFO : topic #0(49.741): 0.158*\"network\" + 0.139*\"algorithm\" + 0.126*\"model\" + 0.117*\"problem\" + 0.117*\"data\" + 0.110*\"method\" + 0.109*\"system\" + 0.109*\"graph\" + 0.096*\"image\" + 0.093*\"time\"\n",
      "2019-03-29 16:37:45,858 : INFO : topic #1(26.031): 0.293*\"channel\" + -0.259*\"image\" + 0.178*\"graph\" + 0.165*\"code\" + 0.151*\"bound\" + -0.145*\"learning\" + -0.138*\"feature\" + -0.121*\"object\" + 0.116*\"capacity\" + 0.112*\"rate\"\n",
      "2019-03-29 16:37:45,865 : INFO : topic #2(24.741): 0.511*\"graph\" + -0.285*\"channel\" + 0.200*\"vertex\" + -0.167*\"user\" + 0.136*\"edge\" + 0.134*\"algorithm\" + -0.115*\"relay\" + -0.110*\"interference\" + 0.109*\"problem\" + -0.105*\"energy\"\n",
      "2019-03-29 16:37:45,874 : INFO : topic #3(21.827): 0.334*\"code\" + 0.278*\"image\" + -0.194*\"network\" + 0.175*\"channel\" + -0.165*\"node\" + -0.164*\"graph\" + -0.152*\"user\" + -0.135*\"social\" + -0.124*\"service\" + 0.122*\"matrix\"\n",
      "2019-03-29 16:37:45,881 : INFO : topic #4(20.766): -0.429*\"graph\" + -0.266*\"network\" + -0.260*\"image\" + 0.198*\"code\" + -0.190*\"node\" + -0.148*\"vertex\" + -0.141*\"channel\" + -0.140*\"edge\" + 0.139*\"game\" + 0.134*\"quantum\"\n",
      "2019-03-29 16:37:53,183 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:37:54,169 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:37:54,170 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:37:55,165 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:38:10,845 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:38:11,652 : INFO : computing the final decomposition\n",
      "2019-03-29 16:38:11,653 : INFO : keeping 200 factors (discarding 16.583% of energy spectrum)\n",
      "2019-03-29 16:38:12,229 : INFO : merging projections: (165200, 200) + (165200, 200)\n",
      "2019-03-29 16:38:16,091 : INFO : keeping 200 factors (discarding 4.496% of energy spectrum)\n",
      "2019-03-29 16:38:17,237 : INFO : processed documents up to #180000\n",
      "2019-03-29 16:38:17,247 : INFO : topic #0(52.983): 0.159*\"network\" + 0.134*\"algorithm\" + 0.131*\"model\" + 0.121*\"data\" + 0.114*\"problem\" + 0.114*\"method\" + 0.107*\"system\" + 0.105*\"graph\" + 0.104*\"image\" + 0.104*\"learning\"\n",
      "2019-03-29 16:38:17,255 : INFO : topic #1(27.754): -0.284*\"image\" + 0.232*\"channel\" + 0.220*\"graph\" + -0.153*\"learning\" + 0.148*\"bound\" + 0.141*\"code\" + -0.139*\"feature\" + -0.125*\"deep\" + -0.125*\"object\" + -0.118*\"neural\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-29 16:38:17,262 : INFO : topic #2(25.705): 0.505*\"graph\" + -0.300*\"channel\" + 0.192*\"vertex\" + -0.176*\"user\" + 0.131*\"edge\" + 0.124*\"algorithm\" + -0.116*\"relay\" + -0.114*\"interference\" + -0.114*\"energy\" + -0.113*\"rate\"\n",
      "2019-03-29 16:38:17,273 : INFO : topic #3(22.913): 0.312*\"image\" + 0.283*\"code\" + 0.217*\"channel\" + -0.146*\"user\" + -0.137*\"agent\" + -0.134*\"social\" + -0.127*\"service\" + -0.125*\"network\" + -0.124*\"system\" + -0.120*\"game\"\n",
      "2019-03-29 16:38:17,281 : INFO : topic #4(21.862): -0.463*\"graph\" + -0.310*\"network\" + -0.217*\"node\" + -0.206*\"image\" + 0.201*\"code\" + -0.154*\"vertex\" + -0.148*\"edge\" + 0.130*\"quantum\" + -0.118*\"channel\" + 0.108*\"game\"\n",
      "2019-03-29 16:38:26,360 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:38:27,506 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:38:27,508 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:38:28,379 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:38:42,669 : INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "2019-03-29 16:38:43,540 : INFO : computing the final decomposition\n",
      "2019-03-29 16:38:43,541 : INFO : keeping 200 factors (discarding 17.050% of energy spectrum)\n",
      "2019-03-29 16:38:44,167 : INFO : merging projections: (165200, 200) + (165200, 200)\n",
      "2019-03-29 16:38:48,815 : INFO : keeping 200 factors (discarding 3.589% of energy spectrum)\n",
      "2019-03-29 16:38:50,218 : INFO : processed documents up to #200000\n",
      "2019-03-29 16:38:50,227 : INFO : topic #0(55.933): 0.160*\"network\" + 0.135*\"model\" + 0.131*\"algorithm\" + 0.123*\"data\" + 0.116*\"method\" + 0.112*\"problem\" + 0.111*\"learning\" + 0.110*\"image\" + 0.107*\"system\" + 0.102*\"graph\"\n",
      "2019-03-29 16:38:50,237 : INFO : topic #1(29.230): -0.302*\"image\" + 0.231*\"graph\" + 0.201*\"channel\" + -0.155*\"learning\" + 0.145*\"bound\" + -0.139*\"feature\" + -0.133*\"deep\" + 0.130*\"code\" + -0.127*\"object\" + -0.123*\"neural\"\n",
      "2019-03-29 16:38:50,246 : INFO : topic #2(26.555): 0.505*\"graph\" + -0.292*\"channel\" + 0.187*\"vertex\" + -0.182*\"user\" + 0.126*\"edge\" + -0.118*\"energy\" + 0.117*\"algorithm\" + -0.113*\"scheme\" + -0.112*\"rate\" + -0.111*\"power\"\n",
      "2019-03-29 16:38:50,255 : INFO : topic #3(23.991): 0.336*\"image\" + 0.243*\"channel\" + 0.241*\"code\" + -0.154*\"agent\" + -0.135*\"system\" + -0.128*\"user\" + -0.123*\"social\" + -0.123*\"service\" + -0.115*\"game\" + 0.103*\"bound\"\n",
      "2019-03-29 16:38:50,265 : INFO : topic #4(22.877): -0.471*\"graph\" + -0.333*\"network\" + -0.233*\"node\" + 0.204*\"code\" + -0.161*\"image\" + -0.150*\"vertex\" + -0.150*\"edge\" + 0.124*\"quantum\" + 0.117*\"language\" + 0.100*\"logic\"\n",
      "2019-03-29 16:38:52,178 : INFO : preparing a new chunk of documents\n",
      "2019-03-29 16:38:52,436 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-03-29 16:38:52,437 : INFO : 1st phase: constructing (165200, 300) action matrix\n",
      "2019-03-29 16:38:52,712 : INFO : orthonormalizing (165200, 300) action matrix\n",
      "2019-03-29 16:39:05,181 : INFO : 2nd phase: running dense svd on (300, 5912) matrix\n",
      "2019-03-29 16:39:05,601 : INFO : computing the final decomposition\n",
      "2019-03-29 16:39:05,602 : INFO : keeping 200 factors (discarding 17.775% of energy spectrum)\n",
      "2019-03-29 16:39:06,182 : INFO : merging projections: (165200, 200) + (165200, 200)\n",
      "2019-03-29 16:39:10,049 : INFO : keeping 200 factors (discarding 1.833% of energy spectrum)\n",
      "2019-03-29 16:39:11,148 : INFO : processed documents up to #205912\n",
      "2019-03-29 16:39:11,156 : INFO : topic #0(56.593): 0.159*\"network\" + 0.134*\"model\" + 0.131*\"algorithm\" + 0.122*\"data\" + 0.115*\"method\" + 0.112*\"problem\" + 0.109*\"learning\" + 0.108*\"image\" + 0.107*\"system\" + 0.103*\"graph\"\n",
      "2019-03-29 16:39:11,163 : INFO : topic #1(29.633): -0.294*\"image\" + 0.223*\"graph\" + 0.211*\"channel\" + -0.155*\"learning\" + 0.150*\"bound\" + 0.149*\"code\" + -0.138*\"feature\" + -0.130*\"deep\" + -0.125*\"object\" + -0.121*\"neural\"\n",
      "2019-03-29 16:39:11,170 : INFO : topic #2(26.940): 0.502*\"graph\" + -0.302*\"channel\" + 0.184*\"vertex\" + -0.178*\"user\" + 0.125*\"edge\" + 0.119*\"algorithm\" + -0.116*\"rate\" + -0.114*\"scheme\" + -0.113*\"energy\" + -0.110*\"relay\"\n",
      "2019-03-29 16:39:11,177 : INFO : topic #3(24.381): 0.331*\"image\" + 0.296*\"code\" + 0.226*\"channel\" + -0.154*\"agent\" + -0.135*\"system\" + -0.135*\"user\" + -0.123*\"service\" + -0.119*\"social\" + -0.114*\"game\" + 0.105*\"bound\"\n",
      "2019-03-29 16:39:11,185 : INFO : topic #4(23.276): -0.457*\"graph\" + -0.317*\"network\" + 0.261*\"code\" + -0.224*\"node\" + -0.167*\"image\" + 0.166*\"quantum\" + -0.147*\"vertex\" + -0.145*\"edge\" + 0.135*\"language\" + 0.112*\"logic\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus = tfidf_corpus, id2word = dictionary, num_topics = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_corpus = lsi[tfidf_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-29 16:39:11,242 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-03-29 16:39:11,244 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'similarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1bc324839b87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindex_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tmpfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsi_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Indexing the lsi_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'similarities' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "index_temp = get_tmpfile(\"index\")\n",
    "index = similarities.Similarity(index_temp, lsi_corpus, num_features = len(dictionary)) #Indexing the lsi_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save('lsi.index')\n",
    "index = similarities.Similarity.load('lsi.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    #Lowering case the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Removing numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    #Removing words of length 1\n",
    "    text = re.sub(r'\\b\\w{1}\\b', '', text)\n",
    "    \n",
    "    #Removing apostrophes\n",
    "    text = text.replace('”', \"\")\n",
    "    text = text.replace('“', \"\")\n",
    "    text = text.replace('-', \" \")\n",
    "    \n",
    "    #Removing punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #Removing stop words\n",
    "    tokens = word_tokenize(text)\n",
    "    text = ' '.join([ i for i in tokens if not i in stopwords])\n",
    "    \n",
    "    \n",
    "    #Stemming and Lemmatizing\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    text = ' '.join([lemmatizer.lemmatize(i) for i in tokens])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk\\'s latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk\\'s representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk\\'s representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.'\n",
    "#query = preprocess_dict(query)\n",
    "query = dictionary.doc2bow(preprocess(query).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aman/anaconda3/lib/python3.7/site-packages/gensim/similarities/docsim.py:528: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  result = numpy.hstack(shard_results)\n",
      "2019-03-29 15:36:11,386 : INFO : loading SparseMatrixSimilarity object from /tmp/index.0\n",
      "2019-03-29 15:36:11,806 : INFO : loaded /tmp/index.0\n",
      "2019-03-29 15:36:11,861 : INFO : loading SparseMatrixSimilarity object from /tmp/index.1\n",
      "2019-03-29 15:36:12,278 : INFO : loaded /tmp/index.1\n",
      "2019-03-29 15:36:12,322 : INFO : loading SparseMatrixSimilarity object from /tmp/index.2\n",
      "2019-03-29 15:36:12,757 : INFO : loaded /tmp/index.2\n",
      "2019-03-29 15:36:12,803 : INFO : loading SparseMatrixSimilarity object from /tmp/index.3\n",
      "2019-03-29 15:36:13,296 : INFO : loaded /tmp/index.3\n",
      "2019-03-29 15:36:13,342 : INFO : loading SparseMatrixSimilarity object from /tmp/index.4\n",
      "2019-03-29 15:36:13,822 : INFO : loaded /tmp/index.4\n",
      "2019-03-29 15:36:13,869 : INFO : loading SparseMatrixSimilarity object from /tmp/index.5\n",
      "2019-03-29 15:36:14,317 : INFO : loaded /tmp/index.5\n",
      "2019-03-29 15:36:14,366 : INFO : loading SparseMatrixSimilarity object from /tmp/index.6\n",
      "2019-03-29 15:36:14,486 : INFO : loaded /tmp/index.6\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_query = tfidf[query]\n",
    "lsi_query = lsi[tfidf_query]\n",
    "sims = index[lsi_query]\n",
    "#print(list(enumerate(sims)))\n",
    "sorted(enumerate(sims), key = lambda item : -item[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
